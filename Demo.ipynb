{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import job_scraper\n",
    "from job_scraper import find_jobs_from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting jobs from Indeed.com or Hinge Health website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_characs = ['titles', 'companies', 'links', 'date_listed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_jobs_from('Indeed', 'data analyst', 'california', desired_characs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_characs = ['titles','links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 new job postings retrieved from Hinge. Stored in results2.xls.\n"
     ]
    }
   ],
   "source": [
    "##compare with new function:\n",
    "find_jobs_from('Hinge', '', '', desired_characs, filename = 'results2.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting jobs from Hinge Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hinge_jobs_div(commitment='Full-time', team='Engineering'):\n",
    "    getVars = {'commitment': commitment, 'team': team}\n",
    "    url = ('https://jobs.lever.co/hingehealth?' + urllib.parse.urlencode(getVars))\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    # job_soup = soup.find(id=\"resultsCol\")\n",
    "    # job_soup = soup.find(data-qa=\"posting-name\")\n",
    "#     job_soup = soup.find_all('h5') ##this returns all the job titles/names, but try to get the LINKS posted as well\n",
    "#     job_soup = soup.find_all('a', class_=\"posting-title\")##trying to get the link for job\n",
    "    job_soup = soup.find(class_=\"postings-group\")\n",
    "    return job_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_soup = load_hinge_jobs_div(commitment='Full-time', team='Engineering') ## find_all funct returns a <class 'bs4.element.ResultSet'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##extract info from soup\n",
    "def extract_job_information_hinge(job_soup, desired_characs):\n",
    "    job_elems = job_soup.find_all('a', class_='posting-title')\n",
    "\n",
    "    cols = []\n",
    "    extracted_info = []\n",
    "\n",
    "    if 'titles' in desired_characs:\n",
    "        titles = []\n",
    "        cols.append('titles')\n",
    "        for job_elem in job_elems:\n",
    "            titles.append(extract_job_title_hinge(job_elem))\n",
    "        extracted_info.append(titles)\n",
    "\n",
    "#     if 'companies' in desired_characs:\n",
    "#         companies = []\n",
    "#         cols.append('companies')\n",
    "#         for job_elem in job_elems:\n",
    "#             companies.append(extract_company_indeed(job_elem))\n",
    "#         extracted_info.append(companies)\n",
    "\n",
    "    if 'links' in desired_characs:\n",
    "        links = []\n",
    "        cols.append('links')\n",
    "        for job_elem in job_elems:\n",
    "            links.append(extract_link_hinge(job_elem))\n",
    "        extracted_info.append(links)\n",
    "\n",
    "#     if 'date_listed' in desired_characs:\n",
    "#         dates = []\n",
    "#         cols.append('date_listed')\n",
    "#         for job_elem in job_elems:\n",
    "#             dates.append(extract_date_indeed(job_elem))\n",
    "#         extracted_info.append(dates)\n",
    "\n",
    "    jobs_list = {}\n",
    "\n",
    "    for j in range(len(cols)):\n",
    "        jobs_list[cols[j]] = extracted_info[j]\n",
    "\n",
    "    num_listings = len(extracted_info[0])\n",
    "\n",
    "    return jobs_list, num_listings\n",
    "\n",
    "def extract_job_title_hinge(job_elem):\n",
    "    title_elem = job_elem.find('h5')\n",
    "    title = title_elem.text.strip()\n",
    "    return title\n",
    "\n",
    "def extract_link_hinge(job_elem):\n",
    "    link = job_elem['href'] ##basically found the element in soup that gave you link. its tag = a, value = href\n",
    "    return link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_elem\n",
    "# job_elem['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_soup.find('a')['href'] ##found element wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now test the whole extract_job_information_hinge function\n",
    "desired_characs = ['titles', 'links']\n",
    "jobs_list, num_listings = extract_job_information_hinge(load_hinge_jobs_div(),desired_characs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##compare results to the indeed query/function\n",
    "load_hinge_jobs_div()\n",
    "type(load_hinge_jobs_div())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indeed_jobs_div(job_title, location):\n",
    "    getVars = {'q': job_title, 'l': location, 'fromage': 'last', 'sort': 'date'}\n",
    "    url = ('https://www.indeed.com/jobs?' + urllib.parse.urlencode(getVars))\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    job_soup = soup.find(id=\"resultsCol\")\n",
    "    return job_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=\"data scientist\"\n",
    "location = \"california\"\n",
    "load_indeed_jobs_div(job_title,location) ## returns a <class 'bs4.element.Tag'> from \"find\" function\n",
    "# type(load_indeed_jobs_div(job_title,location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting jobs from CWjobs.co.uk (using Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_jobs_from('CWjobs', 'data scientist', 'london', desired_characs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "thesis-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
